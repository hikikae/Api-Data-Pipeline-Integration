# Api-Data-Pipeline-Integration
Uno de los principales objetivos para este proyecto es el enriquecimiento de los datos est√°ticos de Google Maps y Yelp, por medio de API¬¥s.
<br>
<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/API-Data-Pipeline-Integration.png"></p><br>

## ‚ú® Elementos del Proceso
- [Cloud Scheduler](#Scheduler)
- [Cloud Function (Get_API_Data)](#Get)
- [Cloud Storage (Data Lake)](#Storage)
- [Segunda Cloud Function (ETL_API_Function)](#Segunda)
- [Cloud Scheduler](#Scheduler)
- [Cloud Storage (Data Warehouse)](#Warehouse)
- [Slack API (Notificaci√≥n)](#Slack)

### Cloud Scheduler
El proceso comienza definiendo un horario de actualizaci√≥n de los datos para que la carga sea de forma autom√°tica, para ello se utiliz√≥ Cloud Scheduler, este llama a un endpoint HTTP que ejecuta al Cloud Function que carga los datos de las API's.La URL se encuentra en la secci√≥n de activadores.

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/Cloud_Scheduler.gif"></p><br>


### <a href="https://github.com/hikikae/Api-Data-Pipeline-Integration/tree/main/details_review_data"> Cloud Function (Get_API_Data)</a>
El flujo de trabajo de la primer Cloud Function es obtener y almacenar la informaci√≥n relevante de los restaurantes. Para ello, se obtienen primero las coordenadas de ciertas ciudades dentro de los cinco estados con mayor densidad poblacional de los Estados Unidos, mediante Geocoding API. Posteriormente, se utiliza Places API para extraer los datos de restaurantes.

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/details_review_data.gif"></p><br>

Cloud Function es activada mediante una solicitud de HTTP. 

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/details_review_data_trigger.png"></p><br>

Es importante mencionar que tanto la Geocoding API como la Places API son proporcionadas por Google y requieren credenciales de API v√°lidas para su correcto funcionamiento.
<br>

### Cloud Storage (Data Lake) 
Los datos se recolectan en su forma original, sin procesamiento previo, en un formato JSON y son almacenados junto con la data est√°tica en el bucket de Cloud Storage, es decir el Data Lake.  

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/datalake.png"></p> <br>

### <a href="https://github.com/hikikae/Api-Data-Pipeline-Integration/tree/main/details_to_dw_function"> Cloud Function (ETL_API_Function) </a>
En esta Cloud Function se llev√≥ a cabo la transformaci√≥n, limpieza y carga de los datos originales mediante la biblioteca de Pandas. Una vez completado el proceso, los datos se env√≠an a un Bucket de Cloud Storage y se emite una notificaci√≥n en Slack para informar sobre la finalizaci√≥n del mismo.

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/ETL_function.gif"></p> <br>

### Cloud Scheduler
Una hora despu√©s del proceso de ETL se ejecuta una query para a√±adir esa nueva data a la tabla principal que contiene ya, la data est√°tica.

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/scheduler_query.png"></p> <br>

### Cloud Storage (Data Warehouse)
Una vez que se han llevado a cabo los procesos de transformaci√≥n de los datos, √©stos se ponen a disposici√≥n en el bucket del proyecto que es nuestro Datawarehouse. De esta forma, tanto el departamento de Data Analytics como el de Data Science pueden acceder a ellos y utilizarlos para sus respectivos an√°lisis y proyectos. Este enfoque facilita la colaboraci√≥n y el intercambio de informaci√≥n valiosa entre los equipos y contribuye a la toma de decisiones informadas basadas en datos precisos y actualizados.

<br>
<p align=center><img width="80%" src="https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/datawarehouse.png"></p> <br>

### Slack API
La finalidad de la integraci√≥n con Slack es enviar una notificaci√≥n acerca del proceso de carga de datos, con el prop√≥sito de evitar la necesidad de estar monitorizando constantemente la plataforma de GCP.
La URL del webhook para ejecutar la notificaci√≥n es √∫nica y espec√≠fica para cada proyecto.

<br>
<p align=center><img width="80%" src= "https://github.com/hikikae/Api-Data-Pipeline-Integration/blob/main/_src/slack_api.gif"></p><br>

##  üõ†Ô∏è Tecnolog√≠as 
- Google Cloud Plataform (GCP)
- Cloud Scheduler
- Cloud Function
- Cloud Storage
- Pandas
- Slack API
- Python

